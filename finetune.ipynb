{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune FLAN-T5 XL/XXL using DeepSpeed on Amazon SageMaker\n",
    "\n",
    "FLAN-T5, released with the [Scaling Instruction-Finetuned Language Models](https://arxiv.org/pdf/2210.11416.pdf) paper, is an enhanced version of T5 that has been fine-tuned in a mixture of tasks, or simple words, a better T5 model in any aspect. FLAN-T5 outperforms T5 by double-digit improvements for the same number of parameters. Google has open sourced [5 checkpoints available on Hugging Face](https://huggingface.co/models?other=arxiv:2210.11416) ranging from 80M parameter up to 11B parameter.\n",
    "\n",
    "In a previous blog post, we already learned how to [“Fine-tune FLAN-T5 XL/XXL using DeepSpeed & Hugging Face Transformers”](https://www.philschmid.de/fine-tune-flan-t5-deepspeed). In this blog post, we look into how we can integrate DeepSpeed into Amazon SageMaker to allow any practitioners to train those billion parameter size models with a simple API call. Amazon SageMaker managed training allows you to train large language models without having to manage the underlying infrastructure. You can find more information about Amazon SageMaker in the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html).\n",
    "\n",
    "This means we will learn how to fine-tune FLAN-T5 XL & XXL using model parallelism, multiple GPUs, and [DeepSpeed ZeRO](https://www.deepspeed.ai/tutorials/zero/) on Amazon SageMaker.\n",
    "\n",
    "The blog post is structured as follows:\n",
    "1. process dataset and upload to S3\n",
    "2. prepare training script and deepspeed launcher\n",
    "3. Fine-tune FLAN-T5 XXL on Amazon SageMaker\n",
    "\n",
    "before we start, let’s install the required libraries and make sure we have the correct permissions to access S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: transformers==4.26.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (4.26.0)\n",
      "Requirement already satisfied: datasets[s3]==2.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (2.9.0)\n",
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (2.145.0)\n",
      "Requirement already satisfied: wandb in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (0.14.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from transformers==4.26.0) (1.23.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from transformers==4.26.0) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from transformers==4.26.0) (2022.10.31)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from transformers==4.26.0) (21.3)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from transformers==4.26.0) (3.6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from transformers==4.26.0) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from transformers==4.26.0) (4.63.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from transformers==4.26.0) (5.4.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from transformers==4.26.0) (2.28.1)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from datasets[s3]==2.9.0) (3.8.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from datasets[s3]==2.9.0) (2022.11.0)\n",
      "Requirement already satisfied: responses<0.19 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from datasets[s3]==2.9.0) (0.18.0)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from datasets[s3]==2.9.0) (3.2.0)\n",
      "Requirement already satisfied: dill<0.3.7 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from datasets[s3]==2.9.0) (0.3.6)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from datasets[s3]==2.9.0) (1.4.4)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from datasets[s3]==2.9.0) (0.70.14)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from datasets[s3]==2.9.0) (10.0.1)\n",
      "Requirement already satisfied: s3fs in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from datasets[s3]==2.9.0) (0.4.2)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (2.6.2)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (3.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.28 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (1.26.47)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: attrs<23,>=20.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (22.2.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (3.20.2)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sagemaker) (4.13.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: setproctitle in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from wandb) (65.6.3)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from wandb) (4.4.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from wandb) (3.1.31)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: pathtools in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from wandb) (1.18.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.47 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.29.47)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from aiohttp->datasets[s3]==2.9.0) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from aiohttp->datasets[s3]==2.9.0) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from aiohttp->datasets[s3]==2.9.0) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from aiohttp->datasets[s3]==2.9.0) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from aiohttp->datasets[s3]==2.9.0) (4.0.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from aiohttp->datasets[s3]==2.9.0) (2.1.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from packaging>=20.0->transformers==4.26.0) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->transformers==4.26.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->transformers==4.26.0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests->transformers==4.26.0) (3.4)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from jsonschema->sagemaker) (0.19.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pandas->datasets[s3]==2.9.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pandas->datasets[s3]==2.9.0) (2022.7)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers==4.26.0\" \"datasets[s3]==2.9.0\" sagemaker wandb --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarshmellow77\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. process dataset and upload to S3\n",
    "\n",
    "Similar to the [“Fine-tune FLAN-T5 XL/XXL using DeepSpeed & Hugging Face Transformers”](https://www.philschmid.de/fine-tune-flan-t5-deepspeed) we need to prepare a dataset to fine-tune our model. As mentioned in the beginning, we will fine-tune [FLAN-T5-XXL](https://huggingface.co/google/flan-t5-xxl) on the [CNN Dailymail Dataset](https://huggingface.co/datasets/cnn_dailymail). The blog post is not going into detail about the dataset generation. If you want to learn the detailed steps check out the [previous post](https://www.philschmid.de/fine-tune-flan-t5). \n",
    "\n",
    "We define some parameters, which we use throughout the whole example, feel free to adjust it to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# experiment config\n",
    "model_id = \"google/flan-t5-base\" # Hugging Face Model Id\n",
    "dataset_id = \"cnn_dailymail\" # Hugging Face Dataset Id\n",
    "dataset_config = \"3.0.0\" # config/verison of the dataset\n",
    "save_dataset_path = \"data\" # local path to save processed dataset\n",
    "text_column = \"article\" # column of input text is\n",
    "summary_column = \"highlights\" # column of the output text \n",
    "# custom instruct prompt start\n",
    "prompt_template = f\"Summarize the following news article:\\n{{input}}\\nSummary:\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the [previous example](https://www.philschmid.de/fine-tune-flan-t5), we are splitting the processing and training into two separate paths. This allows you to run the preprocessing outside of the managed SageMaker Training job. We process (tokenize) the dataset and upload to s3 and pass it into our managed Training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "ds_train = load_dataset(dataset_id, name=dataset_config, cache_dir=\"./cache/\", split=\"train[:1%]\")\n",
    "ds_val = load_dataset(dataset_id, name=dataset_config, cache_dir=\"./cache/\", split=\"validation[:1%]\")\n",
    "ds_test = load_dataset(dataset_id, name=dataset_config, cache_dir=\"./cache/\", split=\"test[:1%]\")\n",
    "\n",
    "dataset = DatasetDict({\"train\": ds_train, \"validation\": ds_val, \"test\": ds_test})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined a `prompt_template` in our config, which we will use to construct an instruct prompt for better performance of our model. Our `prompt_template` has a “fixed” start and end, and our document is in the middle. This means we need to ensure that the “fixed” template parts + document are not exceeding the max length of the model. Therefore we calculate the max length of our document, which we will later use for padding and truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_lenght = len(tokenizer(prompt_template.format(input=\"\"))[\"input_ids\"])\n",
    "max_sample_length = tokenizer.model_max_length - prompt_lenght\n",
    "print(f\"Prompt length: {prompt_lenght}\")\n",
    "print(f\"Max input length: {max_sample_length}\")\n",
    "\n",
    "# Prompt length: 12\n",
    "# Max input length: 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know now that our documents can be “500” tokens long to fit our `template_prompt` still correctly. In addition to our input, we need to understand better our “target” sequence length meaning and how long are the summarization ins our dataset. Therefore we iterate over the dataset and calculate the max input length (at max 500) and the max target length. (takes a few minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# The maximum total input sequence length after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[text_column], truncation=True), batched=True, remove_columns=[text_column, summary_column])\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "max_source_length = min(max_source_length, max_sample_length)\n",
    "print(f\"Max source length: {max_source_length}\")\n",
    "\n",
    "# The maximum total sequence length for target text after tokenization. \n",
    "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[summary_column], truncation=True), batched=True, remove_columns=[text_column, summary_column])\n",
    "target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
    "# use 95th percentile as max target length\n",
    "max_target_length = int(np.percentile(target_lenghts, 95))\n",
    "print(f\"Max target length: {max_target_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have everything needed to process our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(sample, padding=\"max_length\"):\n",
    "    # created prompted input\n",
    "    inputs = [prompt_template.format(input=item) for item in sample[text_column]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[summary_column], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# process dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=list(dataset[\"train\"].features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we processed the datasets we are going to use the new [FileSystem integration](https://huggingface.co/docs/datasets/filesystems) to upload our dataset to S3. We are using the `sess.default_bucket()`, adjust this if you want to store the dataset in a different S3 bucket. We will use the S3 path later in our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded data to:\n",
      "training dataset to: s3://sagemaker-us-east-1-905847418383/processed/cnn_dailymail/train\n",
      "test dataset to: s3://sagemaker-us-east-1-905847418383/processed/cnn_dailymail/test\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/processed/{dataset_id}/train'\n",
    "tokenized_dataset[\"train\"].save_to_disk(training_input_path)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/processed/{dataset_id}/test'\n",
    "tokenized_dataset[\"test\"].save_to_disk(test_input_path)\n",
    "\n",
    "\n",
    "print(\"uploaded data to:\")\n",
    "print(f\"training dataset to: {training_input_path}\")\n",
    "print(f\"test dataset to: {test_input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. prepare training script and deepspeed launcher\n",
    "\n",
    "Done! The last step before we start training our is to prepare our training script and `deepspeed`. We learned in the introduction that we would leverage the DeepSpeed integration with the Hugging Face Trainer. In the [previous post](https://www.philschmid.de/fine-tune-flan-t5-deepspeed) we used the `deepspeed` launcher to start our training on multiple GPUs. As of today Amazon SageMaker does not support the `deepspeed` launcher. 😒 \n",
    "\n",
    "To overcome this limitation, we need to create a custom launcher [ds_launcher.py](./ds_launcher.py). The launcher is a simple python script, which we will pass to our training script. The launcher will start the real training script with the correct environment variables and parameters. In addition, we need to create a `deepspeed_config.json` to configure our training setup. In the [“Fine-tune FLAN-T5 XL/XXL using DeepSpeed & Hugging Face Transformers”](https://www.philschmid.de/fine-tune-flan-t5-deepspeed) post we created 4 deepspeed configurations for the experiments we ran, including `CPU offloading` and `mixed precision`: \n",
    "\n",
    "- [ds_flan_t5_z3_config.json](./configs/ds_flan_t5_z3_config.json)\n",
    "- [ds_flan_t5_z3_config_bf16.json](./configs/ds_flan_t5_z3_config_bf16.json)\n",
    "- [ds_flan_t5_z3_offload.json](./configs/ds_flan_t5_z3_offload.json)\n",
    "- [ds_flan_t5_z3_offload_bf16.json](./configs/ds_flan_t5_z3_offload_bf16.json)\n",
    "\n",
    "Depending on your setup, you can use those, e.g. if you are running on NVIDIA V100s, you have to use the config without `bf16` since V100 are not support `bfloat16` types. \n",
    "\n",
    "> When fine-tuning `T5` models we cannot use `fp16` since it leads to overflow issues, see: [#4586](https://github.com/huggingface/transformers/issues/4586), [#10830](https://github.com/huggingface/transformers/issues/10830), [#10956](https://github.com/huggingface/transformers/pull/10956)\n",
    "> \n",
    "\n",
    "We are going to use a p4dn.24xlarge AWS EC2 Instance including 8x NVIDIA A100 40GB. This means we can leverage `bf16`, which reduces the memory footprint of the model by almost ~2x, which allows us to train without offloading efficiently. \n",
    "\n",
    "We are going to use the [ds_flan_t5_z3_config_bf16.json](./configs/ds_flan_t5_z3_config_bf16.json). If you are irritated by the `auto` values, check the [documentation](https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/deepspeed#configuration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deepspeed_parameters = {\n",
    "  \"deepspeed\": \"./configs/ds_config_bf16_save_16bit_weights.json\", # deepspeed config file\n",
    "  \"training_script\": \"./scripts/run_seq2seq_deepspeed.py\" # real training script, not entrypoint\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune FLAN-T5 XXL on Amazon SageMaker\n",
    "\n",
    "In addition to our `deepspeed_parameters` we need to define the `training_hyperparameters` for our training script. The `training_hyperparameters` are passed to our `training_script` as CLI arguments with `--key value`. \n",
    "\n",
    "If you want to better understand which batch_size and `deepspeed_config` can work which hardware setup you can check out the [Results & Experiments](https://www.philschmid.de/fine-tune-flan-t5-deepspeed#3-results--experiments) we ran.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "training_hyperparameters={\n",
    "  'model_id': model_id,                                # pre-trained model\n",
    "  'train_dataset_path': '/opt/ml/input/data/training', # path where sagemaker will save training dataset\n",
    "  'test_dataset_path': '/opt/ml/input/data/test',      # path where sagemaker will save test dataset\n",
    "  'epochs': 2,                                         # number of training epochs\n",
    "  'per_device_train_batch_size': 8,                    # batch size for training\n",
    "  'per_device_eval_batch_size': 8,                     # batch size for evaluation\n",
    "  'learning_rate': 1e-4,                               # learning rate used during training\n",
    "  'generation_max_length': max_target_length,          # max length of generated summary\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. The Estimator manages the infrastructure use. \n",
    "SagMaker takes care of starting and managing all the required ec2 instances for us, provides the correct huggingface container, uploads the provided scripts and downloads the data from our S3 bucket into the container at /opt/ml/input/data. Then, it starts the training job by running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job_name = f'huggingface-deepspeed'\n",
    "\n",
    "# model_s3_path=f's3://{sagemaker_session_bucket}/flan-t5-xxl-finetuned/model/'\n",
    "environment = {'MODEL_S3_BUCKET': sagemaker_session_bucket, 'MODEL_S3_DIR': 'flan-t5-base-finetuned/model/'}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'ds_launcher.py',  # deepspeed launcher script\n",
    "    source_dir           = '.',               # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g4dn.12xlarge', # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.26',            # the transformers version used in the training job\n",
    "    pytorch_version      = '1.13',            # the pytorch_version version used in the training job\n",
    "    py_version           = 'py39',            # the python version used in the training job\n",
    "    hyperparameters      = {\n",
    "      **training_hyperparameters,\n",
    "      **deepspeed_parameters\n",
    "    },   # the hyperparameter used for running the training job\n",
    "    environment = environment,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created our `HuggingFace` estimator including the `ds_launcher.py` as `entry_point` and defined our `deepspeed` config and `training_script` in the `deepspeed_parameters`, which we merged with our `training_hyperparameters`. We can now start our training job, with the `.fit()` method passing our S3 path to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-deepspeed-2023-04-12-07-56-58-987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-12 07:57:00 Starting - Starting the training job...\n",
      "2023-04-12 07:57:26 Starting - Preparing the instances for training......\n",
      "2023-04-12 07:58:40 Downloading - Downloading input data\n",
      "2023-04-12 07:58:40 Training - Downloading the training image...............\n",
      "2023-04-12 08:00:51 Training - Training image download completed. Training in progress......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-04-12 08:01:44,181 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-04-12 08:01:44,237 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-12 08:01:44,247 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-04-12 08:01:44,249 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-04-12 08:01:44,481 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers==4.26.0 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (4.26.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets==2.9.0 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate==0.16.0 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (0.16.0)\u001b[0m\n",
      "\u001b[34mCollecting evaluate==0.4.0\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 4.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting deepspeed==0.8.0\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.8.0.tar.gz (749 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 749.9/749.9 kB 61.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (1.11.1)\u001b[0m\n",
      "\u001b[34mCollecting rouge-score\u001b[0m\n",
      "\u001b[34mDownloading rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting nltk\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 66.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting py7zr\u001b[0m\n",
      "\u001b[34mDownloading py7zr-0.20.4-py3-none-any.whl (66 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.3/66.3 kB 16.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting wandb\u001b[0m\n",
      "\u001b[34mDownloading wandb-0.14.2-py3-none-any.whl (2.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 100.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (1.26.70)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.0->-r requirements.txt (line 1)) (2022.10.31)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.0->-r requirements.txt (line 1)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.0->-r requirements.txt (line 1)) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.0->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.0->-r requirements.txt (line 1)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.0->-r requirements.txt (line 1)) (0.13.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.0->-r requirements.txt (line 1)) (3.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.0->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.26.0->-r requirements.txt (line 1)) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets==2.9.0->-r requirements.txt (line 2)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.9/site-packages (from datasets==2.9.0->-r requirements.txt (line 2)) (2023.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7 in /opt/conda/lib/python3.9/site-packages (from datasets==2.9.0->-r requirements.txt (line 2)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from datasets==2.9.0->-r requirements.txt (line 2)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets==2.9.0->-r requirements.txt (line 2)) (11.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from datasets==2.9.0->-r requirements.txt (line 2)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets==2.9.0->-r requirements.txt (line 2)) (1.5.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets==2.9.0->-r requirements.txt (line 2)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from accelerate==0.16.0->-r requirements.txt (line 3)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate==0.16.0->-r requirements.txt (line 3)) (5.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.0->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.0->-r requirements.txt (line 5)) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.8.0->-r requirements.txt (line 5)) (1.10.4)\u001b[0m\n",
      "\u001b[34mCollecting absl-py\u001b[0m\n",
      "\u001b[34mDownloading absl_py-1.4.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 30.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge-score->-r requirements.txt (line 7)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 8)) (8.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 8)) (1.2.0)\u001b[0m\n",
      "\u001b[34mCollecting texttable\u001b[0m\n",
      "\u001b[34mDownloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyzstd>=0.14.4\u001b[0m\n",
      "\u001b[34mDownloading pyzstd-0.15.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (390 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 390.4/390.4 kB 47.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting brotli>=1.0.9\u001b[0m\n",
      "\u001b[34mDownloading Brotli-1.0.9-cp39-cp39-manylinux1_x86_64.whl (357 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 357.2/357.2 kB 69.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting inflate64>=0.3.1\u001b[0m\n",
      "\u001b[34mDownloading inflate64-0.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (92 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.0/93.0 kB 17.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pybcj>=0.6.0\u001b[0m\n",
      "\u001b[34mDownloading pybcj-1.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.6/49.6 kB 14.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pyppmd<1.1.0,>=0.18.1\u001b[0m\n",
      "\u001b[34mDownloading pyppmd-1.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.7/138.7 kB 37.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pycryptodomex>=3.6.6\u001b[0m\n",
      "\u001b[34mDownloading pycryptodomex-3.17-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 98.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting multivolumefile>=0.2.3\u001b[0m\n",
      "\u001b[34mDownloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from wandb->-r requirements.txt (line 10)) (4.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /opt/conda/lib/python3.9/site-packages (from wandb->-r requirements.txt (line 10)) (3.20.2)\u001b[0m\n",
      "\u001b[34mCollecting sentry-sdk>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading sentry_sdk-1.19.1-py2.py3-none-any.whl (199 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.2/199.2 kB 50.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting GitPython!=3.1.29,>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.31-py3-none-any.whl (184 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 184.3/184.3 kB 39.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting docker-pycreds>=0.4.0\u001b[0m\n",
      "\u001b[34mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.9/site-packages (from wandb->-r requirements.txt (line 10)) (1.4.4)\u001b[0m\n",
      "\u001b[34mCollecting pathtools\u001b[0m\n",
      "\u001b[34mDownloading pathtools-0.1.2.tar.gz (11 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from wandb->-r requirements.txt (line 10)) (65.6.3)\u001b[0m\n",
      "\u001b[34mCollecting setproctitle\u001b[0m\n",
      "\u001b[34mDownloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.30.0,>=1.29.70 in /opt/conda/lib/python3.9/site-packages (from boto3->-r requirements.txt (line 11)) (1.29.70)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.9/site-packages (from boto3->-r requirements.txt (line 11)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from boto3->-r requirements.txt (line 11)) (0.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.70->boto3->-r requirements.txt (line 11)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.70->boto3->-r requirements.txt (line 11)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.9.0->-r requirements.txt (line 2)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.9.0->-r requirements.txt (line 2)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.9.0->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.9.0->-r requirements.txt (line 2)) (1.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.9.0->-r requirements.txt (line 2)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.9.0->-r requirements.txt (line 2)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets==2.9.0->-r requirements.txt (line 2)) (2.1.1)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.10-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 16.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.26.0->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.26.0->-r requirements.txt (line 1)) (2022.12.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets==2.9.0->-r requirements.txt (line 2)) (2022.7.1)\u001b[0m\n",
      "\u001b[34mCollecting smmap<6,>=3.0.1\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.0-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed, rouge-score, pathtools\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.8.0-py3-none-any.whl size=752135 sha256=91e2ced14dfe20e17a2ed0348f90859565557ef29bbf923251652214bf50c4ee\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/ba/27/48/edac683ddf3d54d4a619ca460375bd226bf1db4a627d681b7c\u001b[0m\n",
      "\u001b[34mBuilding wheel for rouge-score (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for rouge-score (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=908cf4e9ced183467081a702a45dce257707e0edf1aedf8d53d4616ba44ee6d9\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\u001b[0m\n",
      "\u001b[34mBuilding wheel for pathtools (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for pathtools (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=636b0f421d385f0169c69b7d33a84c31b73011ae1f1584cb70ca7ede33f8846a\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed rouge-score pathtools\u001b[0m\n",
      "\u001b[34mInstalling collected packages: texttable, pathtools, brotli, smmap, setproctitle, sentry-sdk, pyzstd, pyppmd, pycryptodomex, pybcj, nltk, multivolumefile, inflate64, docker-pycreds, absl-py, rouge-score, py7zr, gitdb, deepspeed, GitPython, wandb, evaluate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+06f2048\u001b[0m\n",
      "\u001b[34mUninstalling deepspeed-0.6.1+06f2048:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled deepspeed-0.6.1+06f2048\u001b[0m\n",
      "\u001b[34mSuccessfully installed GitPython-3.1.31 absl-py-1.4.0 brotli-1.0.9 deepspeed-0.8.0 docker-pycreds-0.4.0 evaluate-0.4.0 gitdb-4.0.10 inflate64-0.3.1 multivolumefile-0.2.3 nltk-3.8.1 pathtools-0.1.2 py7zr-0.20.4 pybcj-1.0.1 pycryptodomex-3.17 pyppmd-1.0.0 pyzstd-0.15.6 rouge-score-0.1.2 sentry-sdk-1.19.1 setproctitle-1.3.2 smmap-5.0.0 texttable-1.6.7 wandb-0.14.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.0.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-04-12 08:02:02,863 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-04-12 08:02:02,864 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-04-12 08:02:02,923 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-12 08:02:02,987 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-12 08:02:03,052 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-12 08:02:03,063 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"deepspeed\": \"./configs/ds_config_bf16_save_16bit_weights.json\",\n",
      "        \"epochs\": 2,\n",
      "        \"generation_max_length\": 84,\n",
      "        \"learning_rate\": 0.0001,\n",
      "        \"model_id\": \"google/flan-t5-base\",\n",
      "        \"per_device_eval_batch_size\": 8,\n",
      "        \"per_device_train_batch_size\": 8,\n",
      "        \"test_dataset_path\": \"/opt/ml/input/data/test\",\n",
      "        \"train_dataset_path\": \"/opt/ml/input/data/training\",\n",
      "        \"training_script\": \"./scripts/run_seq2seq_deepspeed.py\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"huggingface-deepspeed-2023-04-12-07-56-58-987\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-905847418383/huggingface-deepspeed-2023-04-12-07-56-58-987/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"ds_launcher\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"ds_launcher.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"deepspeed\":\"./configs/ds_config_bf16_save_16bit_weights.json\",\"epochs\":2,\"generation_max_length\":84,\"learning_rate\":0.0001,\"model_id\":\"google/flan-t5-base\",\"per_device_eval_batch_size\":8,\"per_device_train_batch_size\":8,\"test_dataset_path\":\"/opt/ml/input/data/test\",\"train_dataset_path\":\"/opt/ml/input/data/training\",\"training_script\":\"./scripts/run_seq2seq_deepspeed.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=ds_launcher.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=ds_launcher\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-905847418383/huggingface-deepspeed-2023-04-12-07-56-58-987/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"deepspeed\":\"./configs/ds_config_bf16_save_16bit_weights.json\",\"epochs\":2,\"generation_max_length\":84,\"learning_rate\":0.0001,\"model_id\":\"google/flan-t5-base\",\"per_device_eval_batch_size\":8,\"per_device_train_batch_size\":8,\"test_dataset_path\":\"/opt/ml/input/data/test\",\"train_dataset_path\":\"/opt/ml/input/data/training\",\"training_script\":\"./scripts/run_seq2seq_deepspeed.py\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"huggingface-deepspeed-2023-04-12-07-56-58-987\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-905847418383/huggingface-deepspeed-2023-04-12-07-56-58-987/source/sourcedir.tar.gz\",\"module_name\":\"ds_launcher\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"ds_launcher.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--deepspeed\",\"./configs/ds_config_bf16_save_16bit_weights.json\",\"--epochs\",\"2\",\"--generation_max_length\",\"84\",\"--learning_rate\",\"0.0001\",\"--model_id\",\"google/flan-t5-base\",\"--per_device_eval_batch_size\",\"8\",\"--per_device_train_batch_size\",\"8\",\"--test_dataset_path\",\"/opt/ml/input/data/test\",\"--train_dataset_path\",\"/opt/ml/input/data/training\",\"--training_script\",\"./scripts/run_seq2seq_deepspeed.py\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_DEEPSPEED=./configs/ds_config_bf16_save_16bit_weights.json\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_GENERATION_MAX_LENGTH=84\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=google/flan-t5-base\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_TEST_DATASET_PATH=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DATASET_PATH=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_TRAINING_SCRIPT=./scripts/run_seq2seq_deepspeed.py\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 ds_launcher.py --deepspeed ./configs/ds_config_bf16_save_16bit_weights.json --epochs 2 --generation_max_length 84 --learning_rate 0.0001 --model_id google/flan-t5-base --per_device_eval_batch_size 8 --per_device_train_batch_size 8 --test_dataset_path /opt/ml/input/data/test --train_dataset_path /opt/ml/input/data/training --training_script ./scripts/run_seq2seq_deepspeed.py\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:02:07.863: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-04-12 08:02:07,867 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-04-12 08:02:07,887 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mnum_gpus = 4, num_nodes = 1, current_host = algo-1, rank = 0\u001b[0m\n",
      "\u001b[34mcommand = deepspeed --num_gpus=4 ./scripts/run_seq2seq_deepspeed.py --deepspeed ./configs/ds_config_bf16_save_16bit_weights.json --epochs 2 --generation_max_length 84 --learning_rate 0.0001 --model_id google/flan-t5-base --per_device_eval_batch_size 8 --per_device_train_batch_size 8 --test_dataset_path /opt/ml/input/data/test --train_dataset_path /opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:02:10,109] [WARNING] [runner.py:186:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:02:10,149] [INFO] [runner.py:548:main] cmd = /opt/conda/bin/python3.9 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None ./scripts/run_seq2seq_deepspeed.py --deepspeed ./configs/ds_config_bf16_save_16bit_weights.json --epochs 2 --generation_max_length 84 --learning_rate 0.0001 --model_id google/flan-t5-base --per_device_eval_batch_size 8 --per_device_train_batch_size 8 --test_dataset_path /opt/ml/input/data/test --train_dataset_path /opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:02:12,336] [INFO] [launch.py:135:main] 0 NCCL_DEBUG=WARN\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:02:12,337] [INFO] [launch.py:135:main] 0 NCCL_SOCKET_IFNAME=eth0\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:02:12,337] [INFO] [launch.py:135:main] 0 NCCL_IB_DISABLE=1\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:02:12,337] [INFO] [launch.py:135:main] 0 NCCL_VERSION=2.14.3\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:02:12,337] [INFO] [launch.py:142:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:02:12,337] [INFO] [launch.py:148:main] nnodes=1, num_local_procs=4, node_rank=0\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:02:12,337] [INFO] [launch.py:161:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:02:12,337] [INFO] [launch.py:162:main] dist_world_size=4\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:02:12,337] [INFO] [launch.py:164:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 8.37MB/s]\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: marshmellow77. Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: marshmellow77. Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: marshmellow77. Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: marshmellow77. Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\u001b[0m\n",
      "\u001b[34mwandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\u001b[0m\n",
      "\u001b[34mwandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\u001b[0m\n",
      "\u001b[34mwandb: - Waiting for wandb.init()...\u001b[0m\n",
      "\u001b[34mwandb: - Waiting for wandb.init()...\u001b[0m\n",
      "\u001b[34mwandb: - Waiting for wandb.init()...\u001b[0m\n",
      "\u001b[34mwandb: \\ Waiting for wandb.init()...\u001b[0m\n",
      "\u001b[34mwandb: \\ Waiting for wandb.init()...\u001b[0m\n",
      "\u001b[34mwandb: \\ Waiting for wandb.init()...\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.14.2\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20230412_080221-huggingface-deepspeed-2023-04-12-07-56-58-987-t7dm35-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run hf-sm-ds-0\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/marshmellow77/hf-sagemaker-flan-t5-base-2023-04-12-08-02\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/marshmellow77/hf-sagemaker-flan-t5-base-2023-04-12-08-02/runs/huggingface-deepspeed-2023-04-12-07-56-58-987-t7dm35-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.14.2\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20230412_080221-huggingface-deepspeed-2023-04-12-07-56-58-987-m0m8s9-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run hf-sm-ds-2\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/marshmellow77/hf-sagemaker-flan-t5-base-2023-04-12-08-02\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/marshmellow77/hf-sagemaker-flan-t5-base-2023-04-12-08-02/runs/huggingface-deepspeed-2023-04-12-07-56-58-987-m0m8s9-algo-1\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.14.2\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20230412_080221-huggingface-deepspeed-2023-04-12-07-56-58-987-mnv43k-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run hf-sm-ds-3\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/marshmellow77/hf-sagemaker-flan-t5-base-2023-04-12-08-02\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/marshmellow77/hf-sagemaker-flan-t5-base-2023-04-12-08-02/runs/huggingface-deepspeed-2023-04-12-07-56-58-987-mnv43k-algo-1\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json: 100%|██████████| 2.54k/2.54k [00:00<00:00, 345kB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"spiece.model\";:   0%|          | 0.00/792k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"spiece.model\";: 100%|██████████| 792k/792k [00:00<00:00, 76.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json: 100%|██████████| 2.42M/2.42M [00:00<00:00, 52.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)cial_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 1.15MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 100%|██████████| 1.40k/1.40k [00:00<00:00, 214kB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/990M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:   3%|▎         | 31.5M/990M [00:00<00:03, 288MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:   8%|▊         | 83.9M/990M [00:00<00:02, 377MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  14%|█▍        | 136M/990M [00:00<00:02, 404MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  19%|█▉        | 189M/990M [00:00<00:01, 417MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  24%|██▍       | 241M/990M [00:00<00:01, 425MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  30%|██▉       | 294M/990M [00:00<00:01, 428MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  35%|███▍      | 346M/990M [00:00<00:01, 432MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  40%|████      | 398M/990M [00:00<00:01, 434MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  46%|████▌     | 451M/990M [00:01<00:01, 436MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  51%|█████     | 503M/990M [00:01<00:01, 438MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  56%|█████▌    | 556M/990M [00:01<00:00, 439MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  61%|██████▏   | 608M/990M [00:01<00:00, 440MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  67%|██████▋   | 661M/990M [00:01<00:00, 440MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  72%|███████▏  | 713M/990M [00:01<00:00, 441MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  77%|███████▋  | 765M/990M [00:01<00:00, 443MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  83%|████████▎ | 818M/990M [00:01<00:00, 444MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  88%|████████▊ | 870M/990M [00:02<00:00, 445MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  93%|█████████▎| 923M/990M [00:02<00:00, 446MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";:  98%|█████████▊| 975M/990M [00:02<00:00, 447MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)\"pytorch_model.bin\";: 100%|██████████| 990M/990M [00:02<00:00, 433MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)neration_config.json: 100%|██████████| 147/147 [00:00<00:00, 15.2kB/s]\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:02:30,960] [INFO] [comm.py:657:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.14.2\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20230412_080221-huggingface-deepspeed-2023-04-12-07-56-58-987-z0acyp-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run hf-sm-ds-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/marshmellow77/hf-sagemaker-flan-t5-base-2023-04-12-08-02\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/marshmellow77/hf-sagemaker-flan-t5-base-2023-04-12-08-02/runs/huggingface-deepspeed-2023-04-12-07-56-58-987-z0acyp-algo-1\u001b[0m\n",
      "\u001b[34mSetting `WANDB_LOG_MODEL` from TRUE to `end` instead\u001b[0m\n",
      "\u001b[34mSetting `WANDB_LOG_MODEL` from TRUE to `end` instead\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:02:34,470] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.8.0, git-hash=unknown, git-branch=unknown\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:02:36,581] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py39_cu117/fused_adam...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mDetected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/fused_adam/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module fused_adam...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34m[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_75,code=compute_75 -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o\u001b[0m\n",
      "\u001b[34m[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o\u001b[0m\n",
      "\u001b[34m[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.14397692680359 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.146889448165894 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.148656368255615 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py39_cu117/utils...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.147148370742798 seconds\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:03,475] [INFO] [logging.py:68:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:03,489] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:03,489] [INFO] [utils.py:52:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:03,489] [INFO] [logging.py:68:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/utils/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module utils...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:03,658] [INFO] [utils.py:831:see_memory_usage] Stage 3 initialize beginning\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:03,659] [INFO] [utils.py:832:see_memory_usage] MA 0.93 GB         Max_MA 0.93 GB         CA 0.99 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:03,659] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 21.72 GB, percent = 11.6%\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:03,662] [INFO] [stage3.py:114:__init__] Reduce bucket size 589824\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:03,662] [INFO] [stage3.py:115:__init__] Prefetch bucket size 530841\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34m[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[34m[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 16.26190710067749 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 16.130231142044067 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 16.32995057106018 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 16.329339504241943 seconds\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:19,931] [INFO] [utils.py:831:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:19,932] [INFO] [utils.py:832:see_memory_usage] MA 0.93 GB         Max_MA 0.93 GB         CA 0.99 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:19,932] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 21.71 GB, percent = 11.6%\u001b[0m\n",
      "\u001b[34mParameter Offload: Total persistent parameters: 48384 in 64 params\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:20,142] [INFO] [utils.py:831:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:20,143] [INFO] [utils.py:832:see_memory_usage] MA 0.24 GB         Max_MA 0.95 GB         CA 1.11 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:20,143] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 21.71 GB, percent = 11.6%\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:20,260] [INFO] [utils.py:831:see_memory_usage] Before creating fp16 partitions\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:20,261] [INFO] [utils.py:832:see_memory_usage] MA 0.24 GB         Max_MA 0.24 GB         CA 1.11 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:20,261] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 21.71 GB, percent = 11.6%\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,002] [INFO] [utils.py:831:see_memory_usage] After creating fp16 partitions: 1\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,002] [INFO] [utils.py:832:see_memory_usage] MA 0.23 GB         Max_MA 0.24 GB         CA 0.23 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,003] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 22.0 GB, percent = 11.8%\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,117] [INFO] [utils.py:831:see_memory_usage] Before creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,118] [INFO] [utils.py:832:see_memory_usage] MA 0.23 GB         Max_MA 0.23 GB         CA 0.23 GB         Max_CA 0 GB\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,119] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 22.0 GB, percent = 11.8%\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,231] [INFO] [utils.py:831:see_memory_usage] After creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,232] [INFO] [utils.py:832:see_memory_usage] MA 0.46 GB         Max_MA 0.46 GB         CA 0.47 GB         Max_CA 0 GB\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,232] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 22.0 GB, percent = 11.8%\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,347] [INFO] [utils.py:831:see_memory_usage] Before initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,348] [INFO] [utils.py:832:see_memory_usage] MA 0.46 GB         Max_MA 0.46 GB         CA 0.47 GB         Max_CA 0 GB\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,348] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 22.0 GB, percent = 11.8%\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,463] [INFO] [utils.py:831:see_memory_usage] After initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,464] [INFO] [utils.py:832:see_memory_usage] MA 0.92 GB         Max_MA 1.15 GB         CA 1.16 GB         Max_CA 1 GB\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,464] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 22.0 GB, percent = 11.8%\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,464] [INFO] [stage3.py:382:_setup_for_real_optimizer] optimizer state initialized\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0009143352508544922 seconds\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.001012563705444336 seconds\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0011858940124511719 seconds\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,722] [INFO] [utils.py:831:see_memory_usage] After initializing ZeRO optimizer\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,723] [INFO] [utils.py:832:see_memory_usage] MA 1.16 GB         Max_MA 1.34 GB         CA 2.08 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,723] [INFO] [utils.py:840:see_memory_usage] CPU Virtual Memory:  used = 22.0 GB, percent = 11.8%\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,724] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,724] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,724] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7ff92c9d0bb0>\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,724] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[0.003], mom=[[0.9, 0.999]]\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,725] [INFO] [config.py:1008:print] DeepSpeedEngine configuration:\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,726] [INFO] [config.py:1012:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,726] [INFO] [config.py:1012:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,726] [INFO] [config.py:1012:print]   amp_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,726] [INFO] [config.py:1012:print]   amp_params ................... False\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,726] [INFO] [config.py:1012:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,727] [INFO] [config.py:1012:print]   bfloat16_enabled ............. False\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,727] [INFO] [config.py:1012:print]   checkpoint_parallel_write_pipeline  False\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,727] [INFO] [config.py:1012:print]   checkpoint_tag_validation_enabled  True\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,727] [INFO] [config.py:1012:print]   checkpoint_tag_validation_fail  False\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,727] [INFO] [config.py:1012:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ff9f90c2130>\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,727] [INFO] [config.py:1012:print]   communication_data_type ...... None\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,727] [INFO] [config.py:1012:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,727] [INFO] [config.py:1012:print]   curriculum_enabled_legacy .... False\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,727] [INFO] [config.py:1012:print]   curriculum_params_legacy ..... False\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,727] [INFO] [config.py:1012:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,727] [INFO] [config.py:1012:print]   data_efficiency_enabled ...... False\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,727] [INFO] [config.py:1012:print]   dataloader_drop_last ......... False\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,727] [INFO] [config.py:1012:print]   disable_allgather ............ False\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,728] [INFO] [config.py:1012:print]   dump_state ................... False\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,728] [INFO] [config.py:1012:print]   dynamic_loss_scale_args ...... None\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,728] [INFO] [config.py:1012:print]   eigenvalue_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,728] [INFO] [config.py:1012:print]   eigenvalue_gas_boundary_resolution  1\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,728] [INFO] [config.py:1012:print]   eigenvalue_layer_name ........ bert.encoder.layer\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,728] [INFO] [config.py:1012:print]   eigenvalue_layer_num ......... 0\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,728] [INFO] [config.py:1012:print]   eigenvalue_max_iter .......... 100\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,728] [INFO] [config.py:1012:print]   eigenvalue_stability ......... 1e-06\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,728] [INFO] [config.py:1012:print]   eigenvalue_tol ............... 0.01\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,728] [INFO] [config.py:1012:print]   eigenvalue_verbose ........... False\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,728] [INFO] [config.py:1012:print]   elasticity_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,728] [INFO] [config.py:1012:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,728] [INFO] [config.py:1012:print]   fp16_auto_cast ............... None\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,729] [INFO] [config.py:1012:print]   fp16_enabled ................. False\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,729] [INFO] [config.py:1012:print]   fp16_master_weights_and_gradients  False\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,729] [INFO] [config.py:1012:print]   global_rank .................. 0\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,729] [INFO] [config.py:1012:print]   grad_accum_dtype ............. None\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,729] [INFO] [config.py:1012:print]   gradient_accumulation_steps .. 1\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,729] [INFO] [config.py:1012:print]   gradient_clipping ............ 1.0\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,729] [INFO] [config.py:1012:print]   gradient_predivide_factor .... 1.0\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,729] [INFO] [config.py:1012:print]   initial_dynamic_scale ........ 65536\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,729] [INFO] [config.py:1012:print]   load_universal_checkpoint .... False\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,729] [INFO] [config.py:1012:print]   loss_scale ................... 0\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,729] [INFO] [config.py:1012:print]   memory_breakdown ............. False\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,729] [INFO] [config.py:1012:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x7ff9f90c2250>\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,729] [INFO] [config.py:1012:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,729] [INFO] [config.py:1012:print]   optimizer_legacy_fusion ...... False\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,730] [INFO] [config.py:1012:print]   optimizer_name ............... adamw\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,730] [INFO] [config.py:1012:print]   optimizer_params ............. {'lr': 0.003, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,730] [INFO] [config.py:1012:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,730] [INFO] [config.py:1012:print]   pld_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,730] [INFO] [config.py:1012:print]   pld_params ................... False\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,730] [INFO] [config.py:1012:print]   prescale_gradients ........... False\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,730] [INFO] [config.py:1012:print]   scheduler_name ............... WarmupLR\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,730] [INFO] [config.py:1012:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 0.003, 'warmup_num_steps': 0}\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,730] [INFO] [config.py:1012:print]   sparse_attention ............. None\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,730] [INFO] [config.py:1012:print]   sparse_gradients_enabled ..... False\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,730] [INFO] [config.py:1012:print]   steps_per_print .............. 2000\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,730] [INFO] [config.py:1012:print]   train_batch_size ............. 32\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,731] [INFO] [config.py:1012:print]   train_micro_batch_size_per_gpu  8\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,731] [INFO] [config.py:1012:print]   use_node_local_storage ....... False\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,731] [INFO] [config.py:1012:print]   wall_clock_breakdown ......... False\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,731] [INFO] [config.py:1012:print]   world_size ................... 4\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,731] [INFO] [config.py:1012:print]   zero_allow_untested_optimizer  False\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,731] [INFO] [config.py:1012:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=589824 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=530841 param_persistence_threshold=7680 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,731] [INFO] [config.py:1012:print]   zero_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,731] [INFO] [config.py:1012:print]   zero_optimization_stage ...... 3\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21,731] [INFO] [config.py:997:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\", \n",
      "        \"params\": {\n",
      "            \"lr\": 0.003, \n",
      "            \"betas\": [0.9, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 0.0\n",
      "        }\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"WarmupLR\", \n",
      "        \"params\": {\n",
      "            \"warmup_min_lr\": 0, \n",
      "            \"warmup_max_lr\": 0.003, \n",
      "            \"warmup_num_steps\": 0\n",
      "        }\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09, \n",
      "        \"reduce_bucket_size\": 5.898240e+05, \n",
      "        \"stage3_prefetch_bucket_size\": 5.308416e+05, \n",
      "        \"stage3_param_persistence_threshold\": 7.680000e+03, \n",
      "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
      "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"steps_per_print\": 2.000000e+03, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 8, \n",
      "    \"wall_clock_breakdown\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mNo modifications detected for re-loaded extension module utils, skipping build step...\u001b[0m\n",
      "\u001b[34mLoading extension module utils...\u001b[0m\n",
      "\u001b[34mTime to load utils op: 0.0005922317504882812 seconds\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34mNum examples = 2871\u001b[0m\n",
      "\u001b[34mNum examples = 2871\u001b[0m\n",
      "\u001b[34mNum Epochs = 2\u001b[0m\n",
      "\u001b[34mNum Epochs = 2\u001b[0m\n",
      "\u001b[34mInstantaneous batch size per device = 8\u001b[0m\n",
      "\u001b[34mInstantaneous batch size per device = 8\u001b[0m\n",
      "\u001b[34mTotal train batch size (w. parallel, distributed & accumulation) = 32\u001b[0m\n",
      "\u001b[34mTotal train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34mGradient Accumulation steps = 1\n",
      "  Total optimization steps = 180\u001b[0m\n",
      "\u001b[34mTotal optimization steps = 180\u001b[0m\n",
      "\u001b[34mNumber of trainable parameters = 0\u001b[0m\n",
      "\u001b[34mNumber of trainable parameters = 0\u001b[0m\n",
      "\u001b[34mAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\u001b[0m\n",
      "\u001b[34mAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\u001b[0m\n",
      "\u001b[34m0%|          | 0/180 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21.763: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21.764: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21.766: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21.793 algo-1:385 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21.793 algo-1:384 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21.796 algo-1:383 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21.814 algo-1:384 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21.814 algo-1:385 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21.815 algo-1:384 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21.815 algo-1:385 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21.815 algo-1:384 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21.815 algo-1:385 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21.816 algo-1:384 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21.816 algo-1:385 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21.816 algo-1:384 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21.816 algo-1:385 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21.817 algo-1:383 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21.818 algo-1:383 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21.818 algo-1:383 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21.819 algo-1:383 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21.819 algo-1:383 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21.838: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21.868 algo-1:382 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21.889 algo-1:382 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21.889 algo-1:382 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21.890 algo-1:382 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21.891 algo-1:382 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:03:21.891 algo-1:382 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34mYou're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m1%|          | 1/180 [00:03<09:50,  3.30s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 2/180 [00:11<18:10,  6.13s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 3/180 [00:13<12:05,  4.10s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 4/180 [00:14<09:14,  3.15s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 5/180 [00:16<07:39,  2.62s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 6/180 [00:18<06:41,  2.31s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 7/180 [00:19<06:04,  2.11s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 8/180 [00:21<05:39,  1.97s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 9/180 [00:23<05:22,  1.89s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 10/180 [00:24<05:10,  1.83s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 11/180 [00:26<05:02,  1.79s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 12/180 [00:28<04:56,  1.76s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 13/180 [00:30<04:51,  1.74s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 14/180 [00:31<04:47,  1.73s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 15/180 [00:33<04:44,  1.72s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 16/180 [00:35<04:41,  1.72s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 17/180 [00:36<04:39,  1.71s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 18/180 [00:38<04:37,  1.71s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 19/180 [00:40<04:35,  1.71s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 20/180 [00:42<04:34,  1.72s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 21/180 [00:43<04:32,  1.72s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 22/180 [00:45<04:32,  1.72s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 23/180 [00:47<04:30,  1.73s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 24/180 [00:48<04:29,  1.73s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 25/180 [00:50<04:27,  1.72s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 26/180 [00:52<04:25,  1.73s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 27/180 [00:54<04:24,  1.73s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 28/180 [00:55<04:22,  1.73s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 29/180 [00:57<04:20,  1.73s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 30/180 [00:59<04:19,  1.73s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 31/180 [01:01<04:17,  1.73s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 32/180 [01:02<04:15,  1.73s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 33/180 [01:04<04:14,  1.73s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 34/180 [01:06<04:12,  1.73s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 35/180 [01:07<04:11,  1.73s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 36/180 [01:09<04:08,  1.73s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 37/180 [01:11<04:06,  1.73s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 38/180 [01:13<04:04,  1.72s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 39/180 [01:14<04:03,  1.73s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 40/180 [01:16<04:01,  1.73s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 41/180 [01:18<03:59,  1.73s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 42/180 [01:20<03:58,  1.73s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 43/180 [01:21<03:57,  1.73s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 44/180 [01:23<03:56,  1.74s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 45/180 [01:25<03:54,  1.74s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 46/180 [01:27<03:53,  1.74s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 47/180 [01:28<03:52,  1.75s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 48/180 [01:30<03:49,  1.74s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 49/180 [01:32<03:47,  1.74s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 50/180 [01:33<03:46,  1.74s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 51/180 [01:35<03:44,  1.74s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 52/180 [01:37<03:43,  1.75s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 53/180 [01:39<03:42,  1.75s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 54/180 [01:40<03:40,  1.75s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 55/180 [01:42<03:38,  1.75s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 56/180 [01:44<03:36,  1.75s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 57/180 [01:46<03:34,  1.75s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 58/180 [01:47<03:32,  1.74s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 59/180 [01:49<03:30,  1.74s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 60/180 [01:51<03:29,  1.75s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 61/180 [01:53<03:28,  1.75s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 62/180 [01:54<03:26,  1.75s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 63/180 [01:56<03:25,  1.76s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 64/180 [01:58<03:23,  1.76s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 65/180 [02:00<03:21,  1.76s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 66/180 [02:01<03:19,  1.75s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 67/180 [02:03<03:18,  1.75s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 68/180 [02:05<03:16,  1.75s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 69/180 [02:07<03:14,  1.75s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 70/180 [02:08<03:12,  1.75s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 71/180 [02:10<03:11,  1.75s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 72/180 [02:12<03:09,  1.76s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 73/180 [02:14<03:08,  1.76s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 74/180 [02:16<03:06,  1.76s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 75/180 [02:17<03:05,  1.76s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 76/180 [02:19<03:03,  1.76s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 77/180 [02:21<03:01,  1.76s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 78/180 [02:23<02:59,  1.76s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 79/180 [02:24<02:58,  1.77s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 80/180 [02:26<02:56,  1.77s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 81/180 [02:28<02:55,  1.77s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 82/180 [02:30<02:53,  1.77s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 83/180 [02:31<02:51,  1.77s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 84/180 [02:33<02:50,  1.77s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 85/180 [02:35<02:48,  1.78s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 86/180 [02:37<02:46,  1.78s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 87/180 [02:39<02:45,  1.78s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 88/180 [02:40<02:43,  1.78s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 89/180 [02:42<02:42,  1.78s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 90/180 [02:44<02:32,  1.69s/it]\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 115\u001b[0m\n",
      "\u001b[34mNum examples = 115\u001b[0m\n",
      "\u001b[34mBatch size = 8\u001b[0m\n",
      "\u001b[34mBatch size = 8\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mGenerate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mGenerate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mGenerate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mGenerate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m50%|█████     | 2/4 [00:25<00:25, 12.84s/it]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mGenerate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mGenerate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 3/4 [00:51<00:18, 18.20s/it]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mGenerate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mGenerate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m100%|██████████| 4/4 [01:17<00:00, 21.10s/it]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mINFO:absl:Using default tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:absl:Using default tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:absl:Using default tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:absl:Using default tokenizer.\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 2.4919779300689697, 'eval_rouge1': 22.2881, 'eval_rouge2': 7.4709, 'eval_rougeL': 17.9558, 'eval_rougeLsum': 19.722, 'eval_gen_len': 69.52173913043478, 'eval_runtime': 104.689, 'eval_samples_per_second': 1.098, 'eval_steps_per_second': 0.038, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m50%|█████     | 90/180 [04:28<02:32,  1.69s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 4/4 [01:18<00:00, 21.10s/it]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m51%|█████     | 91/180 [04:30<49:09, 33.14s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 92/180 [04:32<34:48, 23.73s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 93/180 [04:34<24:52, 17.15s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 94/180 [04:36<17:58, 12.55s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 95/180 [04:37<13:12,  9.32s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 96/180 [04:39<09:53,  7.07s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 97/180 [04:41<07:35,  5.49s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 98/180 [04:43<05:59,  4.39s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 99/180 [04:45<04:52,  3.61s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 100/180 [04:46<04:05,  3.07s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 101/180 [04:48<03:32,  2.69s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 102/180 [04:50<03:08,  2.42s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 103/180 [04:52<02:51,  2.23s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 104/180 [04:54<02:39,  2.10s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 105/180 [04:55<02:30,  2.01s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 106/180 [04:57<02:23,  1.95s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 107/180 [04:59<02:18,  1.90s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 108/180 [05:01<02:14,  1.87s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 109/180 [05:03<02:11,  1.85s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 110/180 [05:04<02:08,  1.84s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 111/180 [05:06<02:06,  1.83s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 112/180 [05:08<02:03,  1.82s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 113/180 [05:10<02:01,  1.82s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 114/180 [05:12<01:59,  1.81s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 115/180 [05:13<01:57,  1.81s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 116/180 [05:15<01:55,  1.80s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 117/180 [05:17<01:53,  1.80s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 118/180 [05:19<01:51,  1.80s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 119/180 [05:21<01:50,  1.81s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 120/180 [05:22<01:48,  1.81s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 121/180 [05:24<01:46,  1.81s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 122/180 [05:26<01:44,  1.80s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 123/180 [05:28<01:42,  1.80s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 124/180 [05:30<01:40,  1.80s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 125/180 [05:31<01:38,  1.80s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 126/180 [05:33<01:37,  1.80s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 127/180 [05:35<01:35,  1.80s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 128/180 [05:37<01:33,  1.80s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 129/180 [05:39<01:31,  1.80s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 130/180 [05:40<01:29,  1.80s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 131/180 [05:42<01:28,  1.80s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 132/180 [05:44<01:26,  1.80s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 133/180 [05:46<01:24,  1.80s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 134/180 [05:48<01:22,  1.80s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 135/180 [05:49<01:20,  1.80s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 136/180 [05:51<01:19,  1.80s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 137/180 [05:53<01:17,  1.80s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 138/180 [05:55<01:15,  1.80s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 139/180 [05:57<01:13,  1.80s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 140/180 [05:58<01:12,  1.80s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 141/180 [06:00<01:10,  1.80s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 142/180 [06:02<01:08,  1.80s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 143/180 [06:04<01:06,  1.80s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 144/180 [06:06<01:04,  1.80s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 145/180 [06:07<01:02,  1.80s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 146/180 [06:09<01:00,  1.79s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 147/180 [06:11<00:59,  1.79s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 148/180 [06:13<00:57,  1.79s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 149/180 [06:15<00:55,  1.79s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 150/180 [06:16<00:53,  1.79s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 151/180 [06:18<00:51,  1.79s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 152/180 [06:20<00:50,  1.79s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 153/180 [06:22<00:48,  1.79s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 154/180 [06:23<00:46,  1.79s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 155/180 [06:25<00:44,  1.79s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 156/180 [06:27<00:42,  1.79s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 157/180 [06:29<00:41,  1.79s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 158/180 [06:31<00:39,  1.79s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 159/180 [06:32<00:37,  1.79s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 160/180 [06:34<00:35,  1.79s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 161/180 [06:36<00:33,  1.79s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 162/180 [06:38<00:32,  1.79s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 163/180 [06:40<00:30,  1.79s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 164/180 [06:41<00:28,  1.79s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 165/180 [06:43<00:26,  1.79s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 166/180 [06:45<00:25,  1.79s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 167/180 [06:47<00:23,  1.79s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 168/180 [06:49<00:21,  1.79s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 169/180 [06:50<00:19,  1.79s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 170/180 [06:52<00:17,  1.79s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 171/180 [06:54<00:16,  1.79s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 172/180 [06:56<00:14,  1.79s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 173/180 [06:57<00:12,  1.79s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 174/180 [06:59<00:10,  1.79s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 175/180 [07:01<00:08,  1.79s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 176/180 [07:03<00:07,  1.79s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 177/180 [07:05<00:05,  1.79s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 178/180 [07:06<00:03,  1.79s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 179/180 [07:08<00:01,  1.79s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 180/180 [07:10<00:00,  1.70s/it]\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 115\u001b[0m\n",
      "\u001b[34mNum examples = 115\u001b[0m\n",
      "\u001b[34mBatch size = 8\u001b[0m\n",
      "\u001b[34mBatch size = 8\u001b[0m\n",
      "\u001b[34mGenerate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mGenerate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mGenerate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mGenerate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m50%|█████     | 2/4 [00:25<00:25, 12.99s/it]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mGenerate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mGenerate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 3/4 [00:51<00:18, 18.33s/it]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mGenerate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mGenerate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"use_cache\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m100%|██████████| 4/4 [01:17<00:00, 21.20s/it]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mINFO:absl:Using default tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:absl:Using default tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:absl:Using default tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:absl:Using default tokenizer.\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 2.5329129695892334, 'eval_rouge1': 25.1632, 'eval_rouge2': 8.5672, 'eval_rougeL': 20.203, 'eval_rougeLsum': 22.6509, 'eval_gen_len': 57.40869565217391, 'eval_runtime': 105.228, 'eval_samples_per_second': 1.093, 'eval_steps_per_second': 0.038, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 180/180 [08:55<00:00,  1.70s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 4/4 [01:19<00:00, 21.20s/it]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m{'train_runtime': 535.4581, 'train_samples_per_second': 10.724, 'train_steps_per_second': 0.336, 'train_loss': 2.4506715562608505, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 180/180 [08:55<00:00,  1.70s/it]\u001b[0m\n",
      "\u001b[34mSetting `WANDB_LOG_MODEL` from TRUE to `end` instead\u001b[0m\n",
      "\u001b[34mSetting `WANDB_LOG_MODEL` from TRUE to `end` instead\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/tmpgaxsgluq\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/tmpgaxsgluq\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/tmpgaxsgluq/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/tmpgaxsgluq/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/tmpgaxsgluq/generation_config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/tmpgaxsgluq/generation_config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/tmpgaxsgluq/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/tmpgaxsgluq/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mLogging model artifacts. ...\u001b[0m\n",
      "\u001b[34mLogging model artifacts. ...\u001b[0m\n",
      "\u001b[34mwandb: WARNING Calling wandb.login() after wandb.init() has no effect.\u001b[0m\n",
      "\u001b[34mwandb: WARNING Calling wandb.login() after wandb.init() has no effect.\u001b[0m\n",
      "\u001b[34m100%|██████████| 180/180 [08:55<00:00,  2.98s/it]\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /tmp/output/asset/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in /tmp/output/asset/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /tmp/output/asset/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in /tmp/output/asset/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mCopy vocab file to /tmp/output/asset/spiece.model\u001b[0m\n",
      "\u001b[34mCopy vocab file to /tmp/output/asset/spiece.model\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/output/asset/\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to /tmp/output/asset/\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/output/asset/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/output/asset/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/output/asset/generation_config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in /tmp/output/asset/generation_config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/output/asset/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in /tmp/output/asset/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:12:18,708] [INFO] [engine.py:3500:save_16bit_model] Saving model weights to /tmp/output/asset/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:12:18,708] [INFO] [torch_checkpoint_engine.py:15:save] [Torch] Saving /tmp/output/asset/pytorch_model.bin...\u001b[0m\n",
      "\u001b[34mUploading /tmp/output/asset/spiece.model to flan-t5-base-finetuned/model/spiece.modelUploading /tmp/output/asset/spiece.model to flan-t5-base-finetuned/model/spiece.modelUploading /tmp/output/asset/spiece.model to flan-t5-base-finetuned/model/spiece.model\u001b[0m\n",
      "\u001b[34mUploading /tmp/output/asset/generation_config.json to flan-t5-base-finetuned/model/generation_config.json\u001b[0m\n",
      "\u001b[34mUploading /tmp/output/asset/tokenizer.json to flan-t5-base-finetuned/model/tokenizer.json\u001b[0m\n",
      "\u001b[34mUploading /tmp/output/asset/generation_config.json to flan-t5-base-finetuned/model/generation_config.json\u001b[0m\n",
      "\u001b[34mUploading /tmp/output/asset/generation_config.json to flan-t5-base-finetuned/model/generation_config.json\u001b[0m\n",
      "\u001b[34mUploading /tmp/output/asset/tokenizer.json to flan-t5-base-finetuned/model/tokenizer.json\u001b[0m\n",
      "\u001b[34mUploading /tmp/output/asset/tokenizer.json to flan-t5-base-finetuned/model/tokenizer.json\u001b[0m\n",
      "\u001b[34mUploading /tmp/output/asset/config.json to flan-t5-base-finetuned/model/config.json\u001b[0m\n",
      "\u001b[34mUploading /tmp/output/asset/config.json to flan-t5-base-finetuned/model/config.json\u001b[0m\n",
      "\u001b[34mUploading /tmp/output/asset/config.json to flan-t5-base-finetuned/model/config.json\u001b[0m\n",
      "\u001b[34mUploading /tmp/output/asset/training_args.bin to flan-t5-base-finetuned/model/training_args.bin\u001b[0m\n",
      "\u001b[34mUploading /tmp/output/asset/training_args.bin to flan-t5-base-finetuned/model/training_args.bin\u001b[0m\n",
      "\u001b[34mUploading /tmp/output/asset/tokenizer_config.json to flan-t5-base-finetuned/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mUploading /tmp/output/asset/tokenizer_config.json to flan-t5-base-finetuned/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mUploading /tmp/output/asset/pytorch_model.bin to flan-t5-base-finetuned/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mUploading /tmp/output/asset/pytorch_model.bin to flan-t5-base-finetuned/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:12:19,620] [INFO] [torch_checkpoint_engine.py:17:save] [Torch] Saved /tmp/output/asset/pytorch_model.bin.\u001b[0m\n",
      "\u001b[34mUploading /tmp/output/asset/spiece.model to flan-t5-base-finetuned/model/spiece.model\u001b[0m\n",
      "\u001b[34mUploading /tmp/output/asset/generation_config.json to flan-t5-base-finetuned/model/generation_config.json\u001b[0m\n",
      "\u001b[34mUploading /tmp/output/asset/training_args.bin to flan-t5-base-finetuned/model/training_args.bin\u001b[0m\n",
      "\u001b[34mUploading /tmp/output/asset/tokenizer.json to flan-t5-base-finetuned/model/tokenizer.json\u001b[0m\n",
      "\u001b[34mUploading /tmp/output/asset/config.json to flan-t5-base-finetuned/model/config.json\u001b[0m\n",
      "\u001b[34mUploading /tmp/output/asset/tokenizer_config.json to flan-t5-base-finetuned/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mUploading /tmp/output/asset/training_args.bin to flan-t5-base-finetuned/model/training_args.bin\u001b[0m\n",
      "\u001b[34mUploading /tmp/output/asset/pytorch_model.bin to flan-t5-base-finetuned/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mUploading /tmp/output/asset/tokenizer_config.json to flan-t5-base-finetuned/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mUploading /tmp/output/asset/pytorch_model.bin to flan-t5-base-finetuned/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mUploading /tmp/output/asset/special_tokens_map.json to flan-t5-base-finetuned/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mUploading /tmp/output/asset/special_tokens_map.json to flan-t5-base-finetuned/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mwandb: Waiting for W&B process to finish... (success).\u001b[0m\n",
      "\u001b[34mwandb: Waiting for W&B process to finish... (success).\u001b[0m\n",
      "\u001b[34mwandb: - 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)\u001b[0m\n",
      "\u001b[34mwandb: - 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)\u001b[0m\n",
      "\u001b[34mUploading /tmp/output/asset/special_tokens_map.json to flan-t5-base-finetuned/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mwandb: Waiting for W&B process to finish... (success).\u001b[0m\n",
      "\u001b[34mwandb: \\ 0.006 MB of 0.028 MB uploaded (0.000 MB deduped)\u001b[0m\n",
      "\u001b[34mwandb: \\ 0.006 MB of 0.016 MB uploaded (0.000 MB deduped)\u001b[0m\n",
      "\u001b[34mUploading /tmp/output/asset/special_tokens_map.json to flan-t5-base-finetuned/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mwandb: Waiting for W&B process to finish... (success).\u001b[0m\n",
      "\u001b[34mwandb: | 0.028 MB of 0.028 MB uploaded (0.000 MB deduped)\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run hf-sm-ds-1 at: https://wandb.ai/marshmellow77/hf-sagemaker-flan-t5-base-2023-04-12-08-02/runs/huggingface-deepspeed-2023-04-12-07-56-58-987-z0acyp-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20230412_080221-huggingface-deepspeed-2023-04-12-07-56-58-987-z0acyp-algo-1/logs\u001b[0m\n",
      "\u001b[34mwandb: | 0.010 MB of 0.016 MB uploaded (0.000 MB deduped)\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run hf-sm-ds-2 at: https://wandb.ai/marshmellow77/hf-sagemaker-flan-t5-base-2023-04-12-08-02/runs/huggingface-deepspeed-2023-04-12-07-56-58-987-m0m8s9-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20230412_080221-huggingface-deepspeed-2023-04-12-07-56-58-987-m0m8s9-algo-1/logs\u001b[0m\n",
      "\u001b[34mwandb: - 0.006 MB of 0.016 MB uploaded (0.000 MB deduped)\u001b[0m\n",
      "\u001b[34mwandb: \\ 0.014 MB of 0.016 MB uploaded (0.000 MB deduped)\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run hf-sm-ds-3 at: https://wandb.ai/marshmellow77/hf-sagemaker-flan-t5-base-2023-04-12-08-02/runs/huggingface-deepspeed-2023-04-12-07-56-58-987-mnv43k-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20230412_080221-huggingface-deepspeed-2023-04-12-07-56-58-987-mnv43k-algo-1/logs\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:12:28,963] [INFO] [launch.py:350:main] Process 383 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:12:29,964] [INFO] [launch.py:350:main] Process 384 exits successfully.\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Run history:\u001b[0m\n",
      "\u001b[34mwandb:                   eval/gen_len █▁\u001b[0m\n",
      "\u001b[34mwandb:                      eval/loss ▁█\u001b[0m\n",
      "\u001b[34mwandb:                    eval/rouge1 ▁█\u001b[0m\n",
      "\u001b[34mwandb:                    eval/rouge2 ▁█\u001b[0m\n",
      "\u001b[34mwandb:                    eval/rougeL ▁█\u001b[0m\n",
      "\u001b[34mwandb:                 eval/rougeLsum ▁█\u001b[0m\n",
      "\u001b[34mwandb:                   eval/runtime ▁█\u001b[0m\n",
      "\u001b[34mwandb:        eval/samples_per_second █▁\u001b[0m\n",
      "\u001b[34mwandb:          eval/steps_per_second ▁▁\u001b[0m\n",
      "\u001b[34mwandb:                    train/epoch ▁██\u001b[0m\n",
      "\u001b[34mwandb:              train/global_step ▁██\u001b[0m\n",
      "\u001b[34mwandb:               train/total_flos ▁\u001b[0m\n",
      "\u001b[34mwandb:               train/train_loss ▁\u001b[0m\n",
      "\u001b[34mwandb:            train/train_runtime ▁\u001b[0m\n",
      "\u001b[34mwandb: train/train_samples_per_second ▁\u001b[0m\n",
      "\u001b[34mwandb:   train/train_steps_per_second ▁\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Run summary:\u001b[0m\n",
      "\u001b[34mwandb:                   eval/gen_len 57.4087\u001b[0m\n",
      "\u001b[34mwandb:                      eval/loss 2.53291\u001b[0m\n",
      "\u001b[34mwandb:                    eval/rouge1 25.1632\u001b[0m\n",
      "\u001b[34mwandb:                    eval/rouge2 8.5672\u001b[0m\n",
      "\u001b[34mwandb:                    eval/rougeL 20.203\u001b[0m\n",
      "\u001b[34mwandb:                 eval/rougeLsum 22.6509\u001b[0m\n",
      "\u001b[34mwandb:                   eval/runtime 105.228\u001b[0m\n",
      "\u001b[34mwandb:        eval/samples_per_second 1.093\u001b[0m\n",
      "\u001b[34mwandb:          eval/steps_per_second 0.038\u001b[0m\n",
      "\u001b[34mwandb:                    train/epoch 2.0\u001b[0m\n",
      "\u001b[34mwandb:              train/global_step 180\u001b[0m\n",
      "\u001b[34mwandb:               train/total_flos 822475358208.0\u001b[0m\n",
      "\u001b[34mwandb:               train/train_loss 2.45067\u001b[0m\n",
      "\u001b[34mwandb:            train/train_runtime 535.4581\u001b[0m\n",
      "\u001b[34mwandb: train/train_samples_per_second 10.724\u001b[0m\n",
      "\u001b[34mwandb:   train/train_steps_per_second 0.336\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run hf-sm-ds-0 at: https://wandb.ai/marshmellow77/hf-sagemaker-flan-t5-base-2023-04-12-08-02/runs/huggingface-deepspeed-2023-04-12-07-56-58-987-t7dm35-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20230412_080221-huggingface-deepspeed-2023-04-12-07-56-58-987-t7dm35-algo-1/logs\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:12:31,967] [INFO] [launch.py:350:main] Process 385 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-04-12 08:12:33,969] [INFO] [launch.py:350:main] Process 382 exits successfully.\u001b[0m\n",
      "\u001b[34m2023-04-12 08:12:35,455 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-04-12 08:12:35,456 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-04-12 08:12:35,456 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-04-12 08:13:00 Uploading - Uploading generated training model\n",
      "2023-04-12 08:13:00 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\n",
    "    'training': training_input_path,\n",
    "    'test': test_input_path\n",
    "}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to deploy your model to a SageMaker Endpoint, you can check out the [Deploy FLAN-T5 XXL on Amazon SageMaker](https://www.philschmid.de/deploy-flan-t5-sagemaker) blog."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
